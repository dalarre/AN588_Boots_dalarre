---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


##Homework 05

#[1] Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).

```{r}

library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
names(d)
## This loads the data

```

```{r}
library(lmodel2) ## Load the Model II regression package

hr <- log(d$HomeRange_km2)
bmfm <- log(d$Body_mass_female_mean)

f <- cbind(d, hr, bmfm)

m <- lmodel2(hr ~ bmfm, data = f, range.y = "interval", range.x = "relative", 
    nperm = 1000)
m

b1 <- m$regression.results$Slope[1]
b1 ##Slope

b0 <- m$regression.results$Intercept[1]
b0 ##Intercept

```

#[2] Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each β coefficient.

Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.

How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?

How does the latter compare to the 95% CI estimated from your entire dataset?

```{r}
s1 <- NULL
s2 <- NULL
for (i in 1:1000) {
    s1[[i]] <- sample(f$hr, size = 200, replace = TRUE)
    s2[[i]] <- sample(f$bmfm, size = 200, replace = TRUE)
}
## First, I sample 1000 times from each variable, with a sample size of 200


mm <- NULL
beta1 <- NULL
beta0 <- NULL
for (i in 1:1000) {
  mm[[i]] <- lmodel2(s1[[i]]~s2[[i]],data = f, range.y = "interval", range.x = "relative")
  beta1[i] <- mm[[i]]$regression.results$Slope[1]
  beta0[i] <- mm[[i]]$regression.results$Intercept[1]
}
## Then I run a for loop for doing the model 2 regression of each sample. I get the warning that "No permutation test will be performed" on each regression because if I add a permutation number the loop takes forever to run. I also get all the slopes and intercepts stored as vectors in the variables beta1 and beta0.

```

```{r}

mm[[1]]
mm[[1000]]

mmi <- NULL
for (i in 1:1000) {
  mmi[[i]] <- lm(s1[[i]]~s2[[i]])
}
mmi[[1]]
mmi[[1000]]
## Here I do a test to see if there is a notable difference between using linear model 1 or 2, and the results are pretty much the same in both of them.

```

```{r}

sd0 <- sd(beta0) ## Calculates the estimation of the standard error for β0 coefficient as the standard deviation of its sampling distribution 
sd1 <- sd(beta1) ## Calculates the estimation of the standard error for β1 coefficient as the standard deviation of its sampling distribution 


## Next I calculate the SE and 95% CIs of the coefficients using the whole dataset for comparing them to the ones obtained by bootstrapping, and I collect all the data in a data frame called "table" to have an easy way to compare.

mi <- lm(hr ~ bmfm)
u <- coef(summary(mi))
u <- data.frame(unlist(u))
colnames(u) <- c("Estimate", "SE", "t", "p") ## This contains the data about the coefficients calculated with lm()

ci <- confint(mi, level = 0.95)  # This calculates the CIs for the slope (β1) and the intercept (β0)

t <- data.frame(Bootstrapping_SE = c(sd0, sd1), Bootstrapping_2.5 = c(quantile(beta0, 0.025),quantile(beta1, 0.025)), Bootstrapping_97.5 = c(quantile(beta0, 0.975),quantile(beta1, 0.975))) ## This creates a data frame with all the data obtained by bootstrapping

table <- cbind(u[,1:2],t,ci) ## This gathers the different data frames into a single one
table

## We can see that the standard errors aren't the same but are relatively close. The ones calculated by bootstrapping are higher. 
## However, the CIs are totally different. I don't know if I have done something wrong calculating them or if this result is normal.

```

##EXTRA CREDIT

##Tried to do the extra credit for the final homework

Write a FUNCTION that takes as its arguments a dataframe, “d”, a linear model, “m” (as a character string, e.g., “logHR~logBM”), a user-defined confidence interval level, “conf.level” (with default = 0.95), and a number of bootstrap replicates, “n” (with default = 1000). Your function should return a dataframe that includes: beta coefficient names; beta coefficients, standard errors, and upper and lower CI limits for the linear model based on your entire dataset; and mean beta coefficient estimates, SEs, and CI limits for those coefficients based on your bootstrap.

```{r}
a <- function(d, m1, m2, conf.level = 0.95, n = 1000) {
  ## I separate the two variables of the linear model into m1 and m2 because I don't know how to work with them together in m
  
  a1 <- NULL
  a2 <- NULL
  for (i in 1:n) {
    a1[[i]] <- sample(d$m1, size = 200, replace = TRUE)
    a2[[i]] <- sample(d$m2, size = 200, replace = TRUE)
  } ## Loop for sampling
  
  nn <- NULL
  b1 <- NULL
  b0 <- NULL
  for (i in 1:n) {
    nn[[i]] <- lmodel2(m1 ~ m2,data = d)
    b1[i] <- nn[[i]]$regression.results$Slope[1]
    b0[i] <- nn[[i]]$regression.results$Intercept[1]
  } ## Loop for modelling
  
  se0 <- sd(b0)
  se1 <- sd(b1)
  ## Bootstrapping standard error
  
  ni <- lm(m1 ~ m2)
  v <- coef(summary(ni))
  v <- data.frame(unlist(v))
  colnames(v) <- c("Estimate", "SE", "t", "p") 
  ## Whole dataset estimates and standard errors
  
  confinterval <- confint(ni, level = conf.level) ## Whole dataset CIs
  
  r <- data.frame(Bootstrapping_SE = c(se0, se1), Bootstrapping_2.5 = c(quantile(b0, 0.025),quantile(b1, 0.025)),   Bootstrapping_97.5 = c(quantile(b0, 0.975),quantile(b1, 0.975)))
  
  r2 <- cbind(v[,1:2],r,confinterval)
  row.names(r2) <- c("β0","β1")
  
  return(r2) ## Return the data frame with everything
  
}

```


```{r}
a(d = f, m1 = "Brain_Size_Female_Mean", m2 = "Body_Mass_Female_Mean") ## It gives an error "Error in sample.int(length(x), size, replace, prob) : invalid first argument". The m1 and m2 thing isn't working. I don't know how to call a column of the dataset using a variable instead of specifying the column I want. I'm going to try to do it another way.
```


```{r}
b <- function(d, m1, m2, conf.level = 0.95, n = 1000) {
a1 <- NULL
  for (i in 1:n) {
    a1[[i]] <- sample(1:nrow(d), size = 200, replace = TRUE)
  }
 
  nn <- NULL
  b1 <- NULL
  b0 <- NULL
  for (i in 1:n) {
    nn[[i]] <- lmodel2(d$m1[a1[[i]]] ~ d$m2[a1[[i]]],data = d)
    b1[i] <- nn[[i]]$regression.results$Slope[1]
    b0[i] <- nn[[i]]$regression.results$Intercept[1]
  }
  
  se0 <- sd(b0)
  se1 <- sd(b1)
  
  ni <- lm(m1 ~ m2)
  v <- coef(summary(ni))
  v <- data.frame(unlist(v))
  colnames(v) <- c("Estimate", "SE", "t", "p") 
  
  confinterval <- confint(ni, level = conf.level)
  
  r <- data.frame(Bootstrapping_SE = c(se0, se1), Bootstrapping_2.5 = c(quantile(b0, 0.025),quantile(b1, 0.025)),   Bootstrapping_97.5 = c(quantile(b0, 0.975),quantile(b1, 0.975)))
  
  r2 <- cbind(v[,1:2],r,confinterval)
  row.names(r2) <- c("β0","β1")
  
  return(r2)
}
```


```{r}

b(d = f, m1 = "Brain_Size_Female_Mean", m2 = "Body_Mass_Female_Mean") ## Still not working. I get the error "Error in model.frame.default(formula = d$m1[a1[[i]]] ~ d$m2[a1[[i]]], : invalid type (NULL) for variable 'd$m1[a1[[i]]]'". m1 and m2 are the mistake again. I don't know how to do it.

```


##EXTRA EXTRA CREDIT

Graph each beta value from the linear model and its corresponding mean value, lower CI and upper CI from a bootstrap as a function of number of bootstraps from 10 to 200 by 10s. HINT: the beta value from the linear model will be the same for all bootstraps and the mean beta value may not differ that much!








